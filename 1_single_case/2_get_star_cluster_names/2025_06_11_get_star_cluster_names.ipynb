{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f89e6c4-8c03-4ad0-8988-17f35d2cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 17:53:29.414537: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 17:53:29.414788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 17:53:30.144019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 17:53:31.194444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules:\n",
    "from astropy.io import fits\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80de9915-c500-46a0-8d4e-3a7ebf2ef3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------\n",
      "SINGLE -------------------------------\n",
      "5-im case ----------------------------\n",
      "\n",
      "Params:\n",
      "\n",
      "flag_remove_extremes:  False\n",
      "flag_use_augmented:  False\n",
      "\n",
      "flag_black:  False\n",
      "\n",
      "Normalization by\n",
      "five-images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Flag to remove the young and old clusters:\n",
    "flag_remove_extremes = False # Currently removing younger than 10^7 years and older than 10^9.5\n",
    "\n",
    "# Flag to augment or not:\n",
    "flag_use_augmented = False\n",
    "\n",
    "# If we are blacking out:\n",
    "flag_black = False\n",
    "\n",
    "# Choose the normalization method:\n",
    "norm_by = \"five-images\"  # \"dataset\", \"filter, \"five-images\", single-image\"\n",
    "# normalizing by dataset or by filter makes the weird results because the points values are too different. \n",
    "# very small or very large.\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Directories:\n",
    "\n",
    "# Raw data:\n",
    "dir_data_raw = \"/pool001/vianajr/cluster_ages_1/data/data_raw/raw_phangs_dataset.h5\"\n",
    "\n",
    "# Aux text:\n",
    "txt_extremes = \"yes\" if flag_remove_extremes else \"no\"\n",
    "txt_augment = \"yes\" if flag_use_augmented else \"no\"\n",
    "# Base prefix for the results directory:\n",
    "results_prefix = f\"single_case_5im_remextremes_{txt_extremes}_augment_{txt_augment}_\"\n",
    "\n",
    "# If we do have blackout:\n",
    "if flag_black:\n",
    "    \n",
    "    # Flag to use the inner or outer region of the circle:\n",
    "    flag_black_inner = False # If flag_black_inner is True, we are blacking out the center, if Flase black out outer.\n",
    "    # Define the radius for the blacking:\n",
    "    R = 6\n",
    "\n",
    "    # If inner:\n",
    "    if flag_black_inner: \n",
    "        \n",
    "        # Extension for the paths:\n",
    "        extension = \"inner\"\n",
    "        \n",
    "    # If outer: \n",
    "    else: \n",
    "        # Extension for the paths:\n",
    "        extension = \"outer\"\n",
    "\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Number of models per case to get an average of errors:\n",
    "num_models_per_case = 5\n",
    "\n",
    "# Flag to plot preliminary data visualization:\n",
    "flag_plot_data_viz = True\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Display:\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"--------------------------------------\")\n",
    "print(\"SINGLE -------------------------------\")\n",
    "print(\"5-im case ----------------------------\")\n",
    "print()\n",
    "print(\"Params:\")\n",
    "print()\n",
    "print(\"flag_remove_extremes: \", flag_remove_extremes)\n",
    "print(\"flag_use_augmented: \", flag_use_augmented)\n",
    "print()\n",
    "print(\"flag_black: \", flag_black)\n",
    "if flag_black:\n",
    "    print()\n",
    "    print(\"flag_black_inner: \", flag_black_inner)\n",
    "    print(\"R: \", R)\n",
    "print()\n",
    "print(\"Normalization by\")\n",
    "print(norm_by)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c042577b-61cf-45e1-bb8d-bd9fbeab96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Set the seed for reproducibility\n",
    "random_seed = 15\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)  # If you also want to ensure reproducibility with numpy functions\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Class to read the dataset in the format it is:\n",
    "class ReadPhangsH5:\n",
    "    \n",
    "    def __init__(self, hdf5_filename):\n",
    "        # loading from hdf5 file\n",
    "        with h5py.File(hdf5_filename, \"r\") as hf:\n",
    "            \n",
    "            # Get the cluster ID:\n",
    "            self.cluster_ids = np.array(hf[\"cluster_ids\"], dtype=np.int32)\n",
    "            # Use astype(str) to correctly convert HDF5 string datasets to Python strings, for the galaxy_ids:\n",
    "            self.galaxy_ids = np.array(hf[\"galaxy_ids\"]).astype(str)\n",
    "            # Get the image cutouts:\n",
    "            self.image_cutouts = np.array(hf[\"image_cutouts\"], dtype=np.float32)\n",
    "            # Get the log of the ages:\n",
    "            self.cluster_log_ages = np.array(hf[\"cluster_log_ages\"], dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image cutouts for the instance (5 images)\n",
    "        x = self.image_cutouts[index]\n",
    "\n",
    "        # Get the log of the ages:\n",
    "        y = self.cluster_log_ages[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_cutouts)\n",
    "\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Function to split the input (X) and output (Y) from the dataset\n",
    "def separate_X_Y(dataset):\n",
    "    X = [x for x, _ in dataset]\n",
    "    Y = [y for _, y in dataset]\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Function to blackout a circle from the center:\n",
    "def blackout(images, R, flag_black_inner):\n",
    "    \"\"\"\n",
    "    Apply a circular mask blackout to the center of the images with radius R.\n",
    "    The blackout will be an approximation since the images are 2D matrices.\n",
    "    \n",
    "    :param images: A numpy array of shape (n_samples, 5, 112, 112)\n",
    "    :param R: Radius of the blackout circle (better if it is an odd number)\n",
    "    :param flag_black_inner: If True we are removing the Inner center of the image, else the outer.\n",
    "    :return: Modified images with the center blacked out\n",
    "    \"\"\"\n",
    "    # Dimensions of the images\n",
    "    n_samples, filters, height, width = images.shape # Here shape is: samples, filters, height, widht\n",
    "    \n",
    "    # Center of the images\n",
    "    center_x, center_y = width // 2, height // 2  # For 111x111, this will be 55, 55\n",
    "    \n",
    "    # Create a mask with the same dimensions as the image\n",
    "    y, x = np.ogrid[:height, :width]\n",
    "    \n",
    "    # Correct the radius to be applied in a \"circle\" like pattern\n",
    "    mask = (x - center_x) ** 2 + (y - center_y) ** 2 <= (R ** 2)\n",
    "    \n",
    "    # If we are not blacking out the inner, then we are the outer:\n",
    "    if not flag_black_inner:\n",
    "        mask = np.logical_not(mask)\n",
    "        \n",
    "    # Show the mask:\n",
    "    # plt.imshow(mask)\n",
    "\n",
    "    # Apply the mask to each image in the dataset\n",
    "    for i in range(n_samples):\n",
    "        for j in range(filters):\n",
    "            images[i, j][mask] = 0  # Zero out the masked area\n",
    "        \n",
    "    return images\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Define a function to split the data into tr, vl, and ts sets\n",
    "def split_dataset(N, tr_ratio=0.7, vl_ratio=0.15, seed=42):\n",
    "    \n",
    "    # Initialize:\n",
    "    random.seed(seed)\n",
    "    indices = list(range(N))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    tr_split = int(tr_ratio * N)\n",
    "    vl_split = int((tr_ratio + vl_ratio) * N)\n",
    "    \n",
    "    tr_indices = indices[:tr_split]\n",
    "    vl_indices = indices[tr_split:vl_split]\n",
    "    ts_indices = indices[vl_split:]\n",
    "    \n",
    "    return tr_indices, vl_indices, ts_indices\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Function to augment a dataset by creating 8 versions 4 rotations and reversed 4 totations:\n",
    "def augment_dataset_full(data):\n",
    "    \n",
    "    # The actual augmented data:\n",
    "    aug_data = []\n",
    "    # Reference for the past indexes that they corresponded to:\n",
    "    aug_past_idxs = [] \n",
    "    \n",
    "    for i, (x, y) in enumerate(data):\n",
    "\n",
    "        # Original + 90° rotations\n",
    "        for i in range(4):\n",
    "            \n",
    "            # Rotate 0°, 90°, 180°, 270°\n",
    "            rot_x = np.array([ np.rot90(image, k=i) for image in x ])\n",
    "            \n",
    "            # Increase data:\n",
    "            aug_data.append( (rot_x, y) )  \n",
    "            aug_past_idxs.append(i)\n",
    "        \n",
    "        # Flip the image (up-down flip)\n",
    "        flip_x = np.array([ np.flipud(image) for image in x ])\n",
    "        \n",
    "        # Flipped + 90° rotations\n",
    "        for i in range(4):\n",
    "            # Rotate flipped image\n",
    "            rot_flip_x = np.array([ np.rot90(flip_image, k=i) for flip_image in flip_x ])\n",
    "            # Increase data:\n",
    "            aug_data.append( (rot_flip_x, y) )  \n",
    "            aug_past_idxs.append(i)\n",
    "\n",
    "\n",
    "    return aug_data, aug_past_idxs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2246ca-cdfb-4a81-a62c-f3a0ecf3321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (8651, 5, 112, 112)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Start running the code\n",
    "\n",
    "# Create an instance of the dataset class\n",
    "raw_dataset = ReadPhangsH5(dir_data_raw)\n",
    "\n",
    "# Get X and Y:\n",
    "raw_X, raw_Y = separate_X_Y(raw_dataset)\n",
    "# Display:\n",
    "print(\"X.shape: \", raw_X.shape)\n",
    "print()\n",
    "\n",
    "# Get the list of cluster ids and galaxy ids:\n",
    "raw_clust_ids = raw_dataset.cluster_ids\n",
    "raw_galax_ids = raw_dataset.galaxy_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34af4d31-b549-42fc-b0b4-934bbb640c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove instances where Y is less than 7 or bigger than 9.5 if flag_remove_extremes is True\n",
    "if flag_remove_extremes:\n",
    "    \n",
    "    # Mask:\n",
    "    mask = (raw_Y >= 7) & (raw_Y <= 9.5)\n",
    "    \n",
    "    # Apply mask to get the curated sets:\n",
    "    cur_X = raw_X[mask]\n",
    "    cur_Y = raw_Y[mask]\n",
    "    # Update the ids:\n",
    "    cur_clust_ids = raw_clust_ids[mask]\n",
    "    cur_galax_ids = raw_galax_ids[mask]\n",
    "    \n",
    "    # Display:\n",
    "    print(\"New X.shape after removing young clusters: \", cur_X.shape)\n",
    "    print()\n",
    "\n",
    "# Otherwise simple assignation:\n",
    "else:\n",
    "    # Update the data:\n",
    "    cur_X = raw_X\n",
    "    cur_Y = raw_Y\n",
    "    # Update the ids:\n",
    "    cur_clust_ids = raw_clust_ids\n",
    "    cur_galax_ids = raw_galax_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac42d125-500e-4acc-a8bd-2c9c1539564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'cluster_id': raw_clust_ids,\n",
    "    'galaxy_id': raw_galax_ids\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('using_clusters.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb45bb-357a-4774-93e8-bd6899305d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78473720-e4e4-4d42-a83e-ce2ad8181c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tr, vl, and ts indices\n",
    "tr_indices, vl_indices, ts_indices = split_dataset(len(cur_Y))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# If we are working with the blacked dataset:\n",
    "if flag_black:\n",
    "    \n",
    "    # Get the black set:\n",
    "    blck_X = blackout(cur_X, R, flag_black_inner)\n",
    "    # Group the blacked data together:\n",
    "    blck_dataset = [ [x, y] for x, y in zip(blck_X, cur_Y) ]\n",
    "    # Update the chosen dataset:\n",
    "    chosen_dataset = blck_dataset\n",
    "\n",
    "# If not:\n",
    "else:\n",
    "    \n",
    "    # Group the blacked data together:\n",
    "    normal_dataset = [ [x, y] for x, y in zip(cur_X, cur_Y) ]\n",
    "    # Then simply the raw_dataset:\n",
    "    chosen_dataset = normal_dataset\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Now access the data\n",
    "tr_data = [chosen_dataset[i] for i in tr_indices]\n",
    "vl_data = [chosen_dataset[i] for i in vl_indices]\n",
    "ts_data = [chosen_dataset[i] for i in ts_indices]\n",
    "\n",
    "# Get X and Y for tr, vl, and ts sets using the separate_X_Y function\n",
    "X_tr, Y_tr = separate_X_Y(tr_data)\n",
    "X_vl, Y_vl = separate_X_Y(vl_data)\n",
    "X_ts, Y_ts = separate_X_Y(ts_data)\n",
    "\n",
    "# Print the shapes of the data\n",
    "print(f\"Shape of X_tr: {X_tr.shape}\")\n",
    "print(f\"Shape of X_vl: {X_vl.shape}\")\n",
    "print(f\"Shape of X_ts: {X_ts.shape}\")\n",
    "print()\n",
    "print(f\"Shape of Y_tr: {Y_tr.shape}\")\n",
    "print(f\"Shape of Y_vl: {Y_vl.shape}\")\n",
    "print(f\"Shape of Y_ts: {Y_ts.shape}\")\n",
    "print()\n",
    "\n",
    "# Get the ids for the sets:\n",
    "# Cluster ids:\n",
    "tr_clust_ids = [cur_clust_ids[i] for i in tr_indices]\n",
    "vl_clust_ids = [cur_clust_ids[i] for i in vl_indices]\n",
    "ts_clust_ids = [cur_clust_ids[i] for i in ts_indices]\n",
    "# Galaxy ids:\n",
    "tr_galax_ids = [cur_galax_ids[i] for i in tr_indices]\n",
    "vl_galax_ids = [cur_galax_ids[i] for i in vl_indices]\n",
    "ts_galax_ids = [cur_galax_ids[i] for i in ts_indices]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Regardless of which set we use for training, for evaluation purposes we want to have two subsets of the data:\n",
    "#\n",
    "#     Inner: Data inside the limits, without the extremes (young and old stars)\n",
    "#     Outer: Data in the extremes (excluding inner)\n",
    "\n",
    "# Get the masks:\n",
    "msk_subset_inner = (raw_Y >= 7) & (raw_Y <= 9.5)\n",
    "msk_subset_outer = ~ msk_subset_inner\n",
    "\n",
    "# First, you actually need to get the blacked X of all the instances:\n",
    "if flag_black: treated_all_X = blackout(raw_X, R, flag_black_inner)\n",
    "else: treated_all_X = raw_X\n",
    "\n",
    "# Apply the masks to treated_all_X, raw_Y, clust_ids and galax_ids:\n",
    "subset_outer_X = treated_all_X[msk_subset_outer]\n",
    "subset_inner_X = treated_all_X[msk_subset_inner]\n",
    "\n",
    "subset_outer_Y = raw_Y[msk_subset_outer]\n",
    "subset_inner_Y = raw_Y[msk_subset_inner]\n",
    "\n",
    "subset_outer_clust_ids = raw_clust_ids[msk_subset_outer]\n",
    "subset_inner_clust_ids = raw_clust_ids[msk_subset_inner]\n",
    "\n",
    "subset_outer_galax_ids = raw_galax_ids[msk_subset_outer]\n",
    "subset_inner_galax_ids = raw_galax_ids[msk_subset_inner]\n",
    "\n",
    "# CAREFUL: You must get the test points of each subset, you cannot pick any from the training set.\n",
    "# We will proceed by creating a unique identifier of each instance, then seeing which are both in the ts and the subset.\n",
    "\n",
    "# Function to create a unique identifier for each instance based on the cluster and the galaxy id:\n",
    "def create_unique_ids(clust_ids, galax_ids):\n",
    "    return [str(a) + b for a, b in zip(clust_ids, galax_ids)]\n",
    "\n",
    "# Obtain the unique identifiers\n",
    "ts_unique_ids = create_unique_ids(ts_clust_ids, ts_galax_ids)\n",
    "subset_outer_unique_ids = create_unique_ids(subset_outer_clust_ids, subset_outer_galax_ids)\n",
    "subset_inner_unique_ids = create_unique_ids(subset_inner_clust_ids, subset_inner_galax_ids)\n",
    "\n",
    "# Find matching unique IDs\n",
    "matching_ts_and_outer_ids = set(ts_unique_ids) & set(subset_outer_unique_ids)\n",
    "matching_ts_and_inner_ids = set(ts_unique_ids) & set(subset_inner_unique_ids)\n",
    "\n",
    "# If we are removing the extremes, then matching_ts_and_outer_ids is empty:\n",
    "if flag_remove_extremes: matching_ts_and_outer_ids = set(subset_outer_unique_ids)\n",
    "    \n",
    "# Select 600 points from matching IDs - We specify 600 for the test set, to make sure all comparisons are fair among all cases:\n",
    "matching_ts_and_outer_indices = np.random.choice(list(matching_ts_and_outer_ids), 600, replace=False)\n",
    "matching_ts_and_inner_indices = np.random.choice(list(matching_ts_and_inner_ids), 600, replace=False)\n",
    "\n",
    "# Create boolean masks\n",
    "ts_msk_subset_outer = np.isin(subset_outer_unique_ids, matching_ts_and_outer_indices)\n",
    "ts_msk_subset_inner = np.isin(subset_inner_unique_ids, matching_ts_and_inner_indices)\n",
    "\n",
    "# Finally get the evaluation subsets of both X and Y:\n",
    "ts_subset_outer_X = subset_outer_X[ts_msk_subset_outer]\n",
    "ts_subset_inner_X = subset_inner_X[ts_msk_subset_inner]\n",
    "# Don't forget to ravel the Ys:\n",
    "ts_subset_outer_Y = np.ravel(subset_outer_Y[ts_msk_subset_outer])\n",
    "ts_subset_inner_Y = np.ravel(subset_inner_Y[ts_msk_subset_inner])\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# X and Y:\n",
    "use_X_tr = X_tr\n",
    "use_Y_tr = np.ravel(Y_tr) \n",
    "# The ids:\n",
    "use_tr_clust_ids = tr_clust_ids \n",
    "use_tr_galax_ids = tr_galax_ids\n",
    "\n",
    "\n",
    "# The vl and ts Xs:\n",
    "use_X_vl = X_vl\n",
    "use_X_ts = X_ts\n",
    "# The vl and ts Ys:\n",
    "use_Y_vl = np.ravel(Y_vl)\n",
    "use_Y_ts = np.ravel(Y_ts)\n",
    "# The ids:\n",
    "use_vl_clust_ids = vl_clust_ids\n",
    "use_ts_clust_ids = ts_clust_ids\n",
    "use_vl_galax_ids = vl_galax_ids\n",
    "use_ts_galax_ids = ts_galax_ids\n",
    "\n",
    "\n",
    "# Rearrange the shape of the inputs, so the channels/filters is the last dimension, now (n, 112, 112, 5)\n",
    "use_X_tr = np.stack([use_X_tr[:, i, :, :] for i in range(5)], axis=-1)\n",
    "use_X_vl = np.stack([use_X_vl[:, i, :, :] for i in range(5)], axis=-1)\n",
    "use_X_ts = np.stack([use_X_ts[:, i, :, :] for i in range(5)], axis=-1)\n",
    "\n",
    "ts_subset_outer_X = np.stack([ts_subset_outer_X[:, i, :, :] for i in range(5)], axis=-1)\n",
    "ts_subset_inner_X = np.stack([ts_subset_inner_X[:, i, :, :] for i in range(5)], axis=-1)\n",
    "\n",
    "\n",
    "# Print the shapes of the data\n",
    "print(f\"Shape of use_X_tr: {use_X_tr.shape}\")\n",
    "print(f\"Shape of use_X_vl: {use_X_vl.shape}\")\n",
    "print(f\"Shape of use_X_ts: {use_X_ts.shape}\")\n",
    "print()\n",
    "print(f\"Shape of use_Y_tr: {use_Y_tr.shape}\")\n",
    "print(f\"Shape of use_Y_vl: {use_Y_vl.shape}\")\n",
    "print(f\"Shape of use_Y_ts: {use_Y_ts.shape}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
